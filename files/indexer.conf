# mind that we're now using our own up patterns_dir which is managed by puppet
input {
  amqp {
    # ship logs to the 'rawlogs' fanout queue.
    type	=> "all"
    host	=> "127.0.0.1"
    exchange	=> "rawlogs"
    name	=> "rawlogs_consumer"
  }
}

filter {
 #  grok {
 #    patterns_dir	=> [ "/etc/logstash/grok_patterns" ]
 #    match 		=> [ "wtf" ]
 #    type		=> "syslog"
 #    add_tag		=> 'wtf'
 #    add_field		=> [ "wtf", "%{@message}" ]
 #    break_on_match	=> true
 #    named_captures_only => true
 # } 
  grok {
    patterns_dir	=> [ "/etc/logstash/grok_patterns" ]
    type		=> "syslog"
    pattern		=> "%{SYSLOGLINE}"
    break_on_match	=> true
    named_captures_only => true
  }
  grok {
    patterns_dir	=> [ "/etc/logstash/grok_patterns" ]
    type		=> "apache-access"
    pattern		=> "%{COMBINEDAPACHELOG}"
    break_on_match	=> true
    named_captures_only => true
  }
  date {
    type => "syslog"
    # The 'timestamp' and 'timestamp8601' names are for fields in the
    # logstash event.  The 'SYSLOGLINE' grok pattern above includes a field
    # named 'timestamp' that is set to the normal syslog timestamp if it
    # exists in the event.
    timestamp		=> "MMM  d HH:mm:ss"   # syslog 'day' value can be space-leading
    timestamp		=> "MMM dd HH:mm:ss"
    timestamp8601	=> ISO8601 # Some syslogs use ISO8601 time format
  }
  date {
    type	=> "apache-access"
    timestamp	=> "dd/MMM/yyyy:HH:mm:ss Z"
  }
}

output {
  stdout { }

  # If your elasticsearch server is discoverable with multicast, use this:
  #elasticsearch { }

  # If you can't discover using multicast, set the address explicitly
  elasticsearch {
    host => "127.0.0.1"
  }
}
